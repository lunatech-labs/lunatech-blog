= Recap JFall 2025 - Java And Beyond Java
boyuan-xiao-at-work; TanguySrd
v1.0, 2025-11-07
:title: Recap JFall 2025 - Java And Beyond Java
:imagesdir: ../media/2025-11-07-recap-jfall
:lang: en
:tags: [JFall, Quarkus, reactive programming, virtual thread, threat modeling, risk management, xz, cyber security, social engineering, GPU programming, project babylon, 1brc, unsafe java]

== Introduction
It's the season of fall, which means it's time for JFall! As one of the biggest Java conferences in the Netherlands, JFall 2025 lived up to its reputation just as it did in previous years. Apart from the large number of exhibition booths, the quality and quantity of talks are also amazing. However, many talks were scheduled in the same time slots but in different rooms, since it's only a one-day conference. I'm sure many people also share my frustration at not being able to attend all the talks, especially after personally experiencing the brilliance of these talks. Meanwhile, this is also the reason why every person has a unique experience from JFall. In this blog post, we share our full day at JFall along with some reflections of the talks.

== Stop 1. Reactive Programming and Virtual Threads in Quarkus
The very first talk we attended was the "https://jfall.nl/timetable-2025/#:~:text=Concurrency%20Crossroads%3A%20Choosing%20between%20Reactive%20Programming%20and%20Virtual%20Threads%20in%20Quarkus[_Concurrency Crossroads: Choosing between Reactive Programming and Virtual Threads in Quarkus_]" by our fellow colleague Willem Jan Glerum. Quarkus has never been a stranger at Lunatech, as many of its features, like low resource consumption and live-reloading in dev mode, are favored by many people including me.

[[willemjan]]
.Willen Jan at the end of his talk
image::willem_jan_talk.png[A,800]

In the 50mins talk, Willem showed us how easy it is to refactor blocking IO with Quarkus in his  https://github.com/wjglerum/quarkus-virtual-threads/tree/main[well-structured demos]. Blocking IO can happen when the system is querying an external resource and waiting for the response. The reason why it's called blocking is that the CPU thread is blocked until the response is received and couldn't do anything meaningful other than waiting, thus limited throughput. Reactive Programming effectively resolves this by moving on to process the next request and *react* on the response of the first request when it's ready instead of just waiting. In Quarkus, this can be easily achieved via the `Uni` and `Multi` class in `io.smallrye.mutiny` package.

```Java
// blocking version
@GET
@Path("/simple")
@Transactional
@Produces(MediaType.APPLICATION_JSON)
public BlockingBeverage getBeverage() {
    var beverage = bartender.getFromDraft();
    repository.save(beverage);
    return beverage;
}

// reactive version
@GET
@Path("/simple")
@WithTransaction
@Produces(MediaType.APPLICATION_JSON)
public Uni<ReactiveBeverage> getBeverage() {
    return bartender.getFromDraft().onItem().call(beverage -> repository.save(beverage));
}
```

Something else that can boost the throughput without leveraging Reactive Programming is multithreading, let the thread processing the first request block but have a different thread to process other requests. With https://openjdk.org/jeps/425[Virtual Threads coming into preview in JDK 19], this problem gets further addressed as Virtual Threads aims to scale the 'thread-per-request style' further. And all it takes to do this in Quarkus is as easy as adding one annotation:

```Java
import io.smallrye.common.annotation.RunOnVirtualThread;

@GET
@Path("/simple")
@Transactional
@RunOnVirtualThread   // <- here
@Produces(MediaType.APPLICATION_JSON)
public VirtualBeverage getBeverage() {
    var beverage =  bartender.getFromDraft();
    repository.save(beverage);
    return beverage;
}
```

Virtual Threads differs from the normal platform threads not only in that they are managed by JVM and reduce overheads in creation/destroy but also in the fact that Virtual Threads optimize the IO bound work running on one thread. https://www.baeldung.com/java-virtual-thread-vs-thread#virtual-thread-composition[This is achieved by adopting the reactive programming style under the hood]. Therefore, it makes sense that it's specifically mentioned that Virtual Threads should not be pinned in JEP-425.

While using Mutiny in Quarkus for reactive, efficient I/O is certainly simple, the idea of enabling virtual threads with a single annotation is hard to resist. This becomes even more compelling when youâ€™re not building a greenfield project, but refactoring an existing legacy system. The contrast in how much code you need to rewrite for each approach is striking.


== Stop 2. A door with no locks? Letâ€™s talk about threat modeling
The OWASP (Open Worldwide Application Security Project) top 10 is a report that regularly outlines the 10 most critical security risks to web applications. In its latest version, a newly created category was added and listed at the 4th position. This category was - ðŸ¥- Insecure Design (https://owasp.org/Top10/A04_2021-Insecure_Design/[A04:2021 - Insecure Design]).
This leaves us with a question: How can we make sure to create web applications with a secure design?
BÃ rbara Teruggi, https://es.linkedin.com/in/barbara-teruggi[security architect @ Allianz technology], gave us an answer to this question during her talk. This answer was a concept named Threat Modeling.

Threat modeling consists of a â€œproactive & structured analysis of a system or software designâ€. This implies that you should know your software to identify, prioritize and mitigate threats, in order to reduce their likelihood and impact. 
This exercise can be done at different levels and steps of the development process, however, it makes more sense to do it during the design phase as it is the most efficient timing.
We could think that threat modeling is reserved to cybersecurity experts and that developers can only follow what these experts recommend. In reality, multiple actors can be involved in this process, including developers.

After depicting these key points of threat modeling, BÃ rbara gave us some insight about the jargon so we can have a deeper understanding of what is implied here. Her main point was the STRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of Privilege) methodology. STRIDE allows us to sort potential security threats into six different categories. 
Another point she made was about the modeling approaches, she described three; the asset-centric, â€œWhat do we protect?â€, the attacker-centric, â€œWhat does the attacker want?â€ and the software-centric, â€œWhere is the problem?â€.

The final part of BÃ rbaraâ€™s presentation was entitled â€œWhat could go wrongâ€ and was based on Adam Shostackâ€™s https://shostack.org/blog/four-question-frame/[Four-Question Framework]. The questions are the following:

- What are we working on?
- What can go wrong?
- What are we going to do about it?
- Did we do a good job?

BÃ rbara stressed out the fact that threat modeling is an iterative, continuous process. As software evolves through new features or redesigns, threat modeling must be revisited to address fresh risks. This ongoing process ensures security remains aligned with current realities rather than becoming obsolete documentation.



== Stop 3. XZ: A Story of How Internet Was (Almost) Killed
Right after lunch time starts the last keynote of the day, which was unanimously deemed the best talk of the day by all of us. Reinier Zwitserloot, Roel Spilker and their team presented a thrilling yet gripping story of how the internet was almost killed.

It all begins with Lasse Collin's ambition of creating his own linux distro in the early 2000s. While the dream didn't land, the XZ utils, which was developed to better compress the OS image to fit into a CD-ROM, became a big success. XZ supports outputting both `.xz` format and `.lzma` format. For its fast and significant volume decreasing compression feature, XZ became not only a built-in utils in many popular linux distros but also a build dependency of numerous well-known linux libraries.

Just like many other open source projects, Lasse gradually shifted focus and became less active in maintaining the XZ. Meanwhile, Jia Tan stands out as a strong force of contributor by making several patches for the project in 2021. In the next three years, Jia built up his reputation and picked up the role of maintainer of XZ as Lasse was bothered by a long-term mental health issue. With such power and authority, Jia made multiple commits to culprit the sophisticated backdoor within XZ, including disabling the landlock, uploading malicious script encrypted and hidden in a tar file and finally replacing an authentication function used by OpenSSH using `IFUNC`. If Andres Freund, a PostgresSQL developer at Microsoft, didn't notice the weird performance regression when doing a micro-benchmarking for PostgresDB in 2024, the XZ backdoor would remain undiscovered and Jia Tan, or whoever it is behind that account, could gain access to the vast majority of the servers on the internet.

[[andresfreund]]
.The moment that the attack failed
image::andres_freund.png[A,800]

While we are in the relief of the dooms day is avoided, we need to ask ourselves what we can learn from this. Despite how technically sophisticated https://x.com/fr0gger_/status/1774342248437813525/photo/1[the whole attack process] is, we found the non-technical part, the social engineering part, highly essential as well. On the way from just a random contributor to a core maintainer, Jia received help from other suspicious users like Jigar Kumar and Dennis Ens who sent mail complaint to Lasse to push the merge of Jia's patches. There's no doubt that these pressure catalyzed the turn-over of project's control, which is fundamental to carry out the attack.

We didnâ€™t love this presentation just for its content, but for how Reinier and Roel told the story. With a bit of performance and acting, they brought everything to life as if we were witnessing it ourselves. It also reminded me of the movie https://www.imdb.com/title/tt3042408/[Who Am I], where I first encountered the concept of social engineering: characters compromising their targets through a mix of technical skills and psychological manipulation. That blend is exactly what we should stay mindful of in the real world, too.

== Stop 4. Java's Answer to AI Model And GPU Programming
https://blog.lunatech.com/posts/2025-06-27-gpu-programming-for-the-brave[I have been long interested in the topic of GPU Programming] and I had the pleasure to learn the basics of building simple Neural Network in Python during my university study. But little did I expect to see a talk about them in a Java conference. With strong curiosity, I listened to the talk given by Lize Raes and Ana Maria Mihalceanu, where I see Java's answer to AI modeling and GPU programming. AI models mainly consist of Neural Networks and it was remarkably easy to build Neural Networks using a deep learning framework like Tensorflow or PyTorch. And of course, they are limited to Python only. As for the GPU Programming, you will need to know CUDA or OpenCL first. But with the help of the https://openjdk.org/projects/babylon/[babylon project as part of the OpenJDK], this is now possible in Java.

[[gpu_talk_slide]]
.The slide that compare the three demos during the talk
image::gpu_talk_slide.png[A,800]

Lize and Ana showed us two ways to run a ML model on GPU using Java in their first two demos, both of them relied on https://onnx.ai/[ONNX (Open Neural Network Exchange)]. ONNX defines an open standard for save format of a ML model as well as runtimes to execute the model. In the first demo, Lize and Ana ran an emotion recognition model that is saved as `.onnx` format and then executed in a Java program that leverages https://openjdk.org/jeps/454[the new Foreign Function and Memory API in JDK 22]. As the replacement of the good old JNI (Java Native Interface), Foreign Function eliminates the necessity of C/C++ bridge (something like `JNIEXPORT void JNICALL Java_my_natives_MyNativeClass_myNativeFunc(JNIEnv *, jobject, jint);`) and provides a concise, readable, pure-Java API. 

The second approach, which gives us more Tensorflow/PyTorch flavor of creating a model, heavily leverages the `@CodeReflection` annotation introduced by project babylon. By placing the `@CodeReflection` annotation on top of the method, java source code can be identified as code model (also known as Intermediate Representation, IR, in the realm of compilers), which then can be examined or transformed into other forms of interpretation. In project babylon, the developers build up a translation layer for the ONNX operators (Conv, Relu, etc...) using `@CodeReflection`. By doing so, developers can use pure Java code to **program** Neural Networks that can run on a actual GPU.

[[java_ml_code]]
.Rings a bell, doesn't it? Source: https://github.com/LizeRaes/babylon/blob/a43beadb9347f7ce03d3d7f9b27e83fef5b7cf3f/cr-examples/onnx/src/test/java/oracle/code/onnx/fer/FERModel.java#L116[here]
image::java_ml_code.png[A,800]

Finally in the third demo we saw the possibility of writing GPU kernels in pure Java. With the help of Heterogeneous Accelerator Toolkit (HAT), the kernel context and compute context, which contains the GPU grid shape and thread info, can be easily accessed from the Java code. Different from the previous two demos, where we heavily rely on native runtimes, our Java code can be translated into GPU code, thanks to the `@CodeReflection`. Even though HAT doesn't change the fundamental programming model, where you have to always consider the fact that your code will be run in parallel in thousands of GPU threads, it does provide multiple backend so that your Java GPU code can run on multiple providers. Except for CUDA and OpenCL, it's even possible to run on JDK itself, in other words, using Java backend to simulate the GPU execution. While we can benefit from the portability of JDK, it also enables us to debug the GPU kernel in an easier way.

[[java_to_gpu_code]]
.The slide that shows how our Java code becomes GPU code.
image::java_to_gpu_code.png[A,800]

== Stop 5. 1brc: The Most Mind-opening Java Code Ever Seen
Pushing Java to the Limits: Processing a Billion Rows in Under 2 Seconds

Another talk we attended, and one that astonished us by its way of turning a simple problem into a performance engineering masterclass. This talk was about â€œThe One Billion Row Challengeâ€ and was given by https://www.linkedin.com/in/royvanrijn/?originalSubdomain=nl[Roy van Rijn], Java Champion, who participated in this challenge.

Roy first explained what exactly was this competition about. Participants were given a file with a billion rows listing stations and their temperature values. They had to parse it (min, max, mean) and then print the results using only plain â€˜vanillaâ€™ java. The goal was obviously to come up with the fastest way of doing so.

The baseline implementation participants were given was 5 minutes long and used â€˜naÃ¯veâ€™ techniques such as `Files.lines` to read (single threaded) and streams to collect the results. This implementation was quickly improved by Roy, who was the first person to submit a solution. Thanks to the use of `Files.lines` in parallel and https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html[ConcurrentHashMap], he was able to reduce the runtime to 2 minutes.

The next step to improve the runtime was switching to a hardware-aware implementation. CPU architecture highly influences the speed of rows treatment, and it didnâ€™t last long until the competitors took advantage of it. These were the major breakthrough at this point:

- Replacing BufferedReader with https://www.geeksforgeeks.org/operating-systems/memory-mapped-files-in-os/[memory-mapped files] to reduce I/O overhead
- Gaining major speed-ups by reading raw bytes directly from mmap
- Discovering that naive parallel streams worsen cache locality

Then came the latest and trickiest tweaks: low-level, high-impact optimizations. First there was the switch from conventional parsing to https://en.algorithmica.org/hpc/pipelining/branchless/[branchless implementation] in order to prevent CPU pipeline stalls.

```Java
// Usual implementation
int a = 5;
int b = 10;
int max;

if (a > b) {
    max = a;
} else {
    max = b;
}

// Branchless implementation
int a = 5;
int b = 10;
int max;

int mask = (a > b) -1;
max = (a & mask) | (b & ~mask);
```

This was followed by replacing if-else logic with bitwise operations to make parsing faster and more predictable. https://en.wikipedia.org/wiki/SWAR[SWAR] techniques were brought in to process multiple bytes at once, significantly reducing CPU stalls and boosting overall throughput. As the parsing layer became leaner, it became clear that Javaâ€™s built-in HashMap couldnâ€™t keep up under this workload, which led to the creation of a custom https://www.baeldung.com/cs/hashing-linear-probing[forward-probing] hash map with a tightly packed memory layout. This eliminated object overhead and pointer chasing, allowing station aggregation to remain just as efficient as the parsing layer feeding into it. Pushing things even further, the https://www.baeldung.com/java-unsafe[Unsafe API] was used to access raw memory directly, trading off some safety and readability for raw speed. By this point, the optimizations had left the realm of everyday Java and entered territory where competitors were pushing the language to its absolute limits in pursuit of every last microsecond.

All along his talk, Roy stressed the importance and the huge place of collaboration and open source during this contest. Everybody was looking at each otherâ€™s code, learning and experimenting which he found really cool (and us too).


== To sum up
Even though JFall 2025 was only a one-day conference, it still impressed us with the sheer quality and variety of its content. We saw how Javaâ€™s capabilities continue to evolve, and we were reminded why cybersecurity truly matters in the real world. While writing this blog post, our understanding of the talks deepened and shifted, which is why we also want to share our own reflections hereâ€”continuing the spirit at the heart of JFall itself: sharing knowledge.