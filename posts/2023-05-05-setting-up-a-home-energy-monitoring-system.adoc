= Setting up a home energy monitoring system
eloots
v1.0, 2023-05-05
:title: Setting up a home energy monitoring system
:imagesdir: ../media/2023-05-05-setting-up-a-home-energy-monitoring-system
:lang: en
:tags: [raspberry pi, emqx, influxdb, smappee, telegraf, grafana, energy, monitoring]

[id=introduction]
== Introduction

So I had been using a https://www.smappee.com/infinity[_Smappee_] system that came with the solar panel installation that was installed back in July 2020 to monitor the electricity generation and consumption in our house for about two years when I noticed something odd. I used the neat _Smappee_ iOS app to check the figures of the energy generated by the solar panels on my roof in various levels of detail; from five-minute averages to amounts per month. The system used to keep granular numbers for more than a month in the past, when, out of the blue, it reduced this period to a mere two weeks. Bummer.

To illustrate the issue, let's look at a couple of sample plots. The first one is an example of 5 minute average production and consumption data for one day (August 23, 2022):

image::23-08.PNG[5 minute averages over one day on August 23rd]

The next one shows the same graphs over a period of 16 days (from August 8th till August 23rd):

image::08-16days.PNG[Production and consumption data over 16 consecutive days]

As you can see in the lower-right corner of the graph, on August 8th, the granularity flips from 5 minutes to an hour.

The obvious question I asked myself was how to keep granular data longer, and I saw two options. I could ask the company behind the system to reinstate the longer data retention period, or investigate if I could build an energy monitoring system that would allow me to configure the data retention policy as I wished it to be. I chose option two, as it seemed to also provide me with an opportunity to learn technologies I had never used before.

Before I found out about the data granularity issue, I had already discovered that the _Smappee_ energy management system was quite open: it provides two ways to get access to the collected data. One method is to use the _Smappee_ REST API, the other is to use https://en.wikipedia.org/wiki/MQTT[_MQTT_].

The REST API can be used to query endpoints in the _Smappee_ Cloud, linked to an individual _Smappee_ monitoring system &#8212;let's refer such a system as a _Smappee_&#8212; located for example in one's house. Use of the API is free for non-commercial use and requires setting up https://oauth.net/2[OAuth2] authentication.

On the other hand, when using _MQTT_, the _Smappee_ monitoring system configuration has to be tweaked to point to a so-called _MQTT Broker_. Once that's done, the _Smappee_ device will send measurement samples at a once per second interval to the _MQTT Broker_.

The main difference between the two access methods, is that the first one implies _polling_ for data in the _Smappee_ Cloud, whereas the second one is _pushing_ the data from the _Smappee_ to a [local] server.

With all this in mind let's have a look at the project goals I set for my Home Energy Monitoring System (_HEMS_).

[id=project-objectives]
== Project objectives

The main objective of the _HEMS_ project was to collect data from one or more _Smappees_  at a specific level of granularity with configurable aggregation and data retention periods and the ability to create dashboards that display selected aggregates over a chosen period of time.

For example, a _Smappee_, when configured to send data to an _MQTT_ Broker sends data at a one second interval. One may want to aggregate this data at intervals of minutes, days, weeks or months where each aggregation level may have specific data retention periods. For example, the high frequency 1 minute aggregates may be retained for several months whereas, on the other side of the spectrum, the monthly averages may be retained indefinitely (and obviously, as long as storage capacity permits).

As I wanted to be independent of external factors (such as the _Smappee_ Cloud) as to not be exposed to the whims of external factors, I decided to choose the _MQTT_ route for building my _HEMS_.

Next to the already cited main objective, there were other objectives I wanted to achieve:

* If possible, implement the system without any programming.
* Use _Open Source Software_ (OSS) only.
* Capture all knowledge gained in building the system as to make it easily reproducible.
* Automate the installation of the system as much as possible.

With the different project objectives set, let's see if and how these were achieved in the following chapters of this article.

== A brief introduction to _Smappee_

_Smappees_ come in different versions. I currently have two at home. The first one, a _Smappee Solar_ came with the solar panel installation and measures the total electrical power consumption of the house and the output power of the solar panels on the roof of the house. The second one is a modular _Smappee Infinity_ that consists of:

- a _Smappee Power box_ that powers all connected _Smappee_ devices and that measures line voltage(s). It can be used in mono-phase and tri-phase power systems. 

- a _Smappee Genius_ which is the brain of the system. This device implements Wifi and physical Ethernet connectivity and acts as a Gateway.

- one or up-to 7 _Smappee CT Hubs_ that each can measures current in up-to 4 electrical circuits.

The _Smappee Power Hub_ is connected to the _Smappee Genius_ which in turn is connected to the daisy chained _CT Hubs_ via https://en.wikipedia.org/wiki/RS-485[RS-485], using 4-wire twisted pair cables. These cables serve two purposes. On the one hand, they transfer power to the different _Smappee_ components and on the other hand, they form the communication channel between the _Smappee_ components.

As mentioned in the introduction, a _Smappee_ performs measurements at a 1 Hz rate. The _Smappee_ genius measures:

- Mains frequency (Hz)
- Mains voltage(s) (V)
- Circuit currents (A)
- Power (real, reactive, and apparent) (W)

The _Smappee Solar_ is a scaled down version of the _Smappee Infinity_ system optimised for Solar panel installations.

Even though the physical installation is best performed by a professional, the configuration can be done by anyone with basic IT skills.

I've had the _Smappees_ in service for 3 years (_Smappee Solar_) and 9 months (_Smappee Infinity_) and both have proven to be very reliable systems.

== _HEMS_ Architecture & Implementation

=== Architecture

As mentioned in the introduction, I opted for _MQTT_ as a means to access the _Smappee_ data measurements.

The documentation has the following to say about using _MQTT_:

image::smappee-mqtt.png[Smappee documentation about using MQTT]

_"Use of MQTT requires programming knowledge."_

Really? With what I had in mind, it would surprise me that this would be the case, so let's have a look at that.

All in all, the _HEMS_ system needs to have the following components:

- A Time-Series database that will store measurements, more specifically aggregated measurements, with an optional data retention time.
- A Display server that will offer various dashboards. The latter can be displayed on any web browser that is pointed to the server.
- An _MQTT Broker_. This server will take in raw measurements published on so-called topics by the _Smappee_ systems and offer these to subscribers.
- The raw Smappee measurements are most likely not in a format that is suitable to write to the time-series database as-is. We may want to only retain data we're interested in and drop other stuff. Also, it is likely that we want to rename certain data fields. Therefore, we need a component that subscribes to topics on the _MQTT Broker_ and that transforms and aggregates the raw measurements and writes it in the Time-series database. Let's call this component the IO/transformer/aggregator.

=== Mapping of the architecture to specific components

There are various alternatives for each of the above components listed in the previous section, but I chose the following (OSS) implementations:

- Time Series Database: https://github.com/influxdata/influxdb[InfluxDB]
- Display server: https://github.com/grafana/grafana[Grafana Server]
- _MQTT Broker_: https://github.com/emqx/emqx[EMQX]
- IO/Transformer/aggregator: https://github.com/influxdata/telegraf[Telegraf]

These components needs some platform to run on and as I have quite a lot of experience with Raspberry Pi, I decided to run everything on a Raspberry Pi using the Ubuntu Server Operating System.

The following diagram shows the overall set-up of the _HEMS_:

image::setup-rpi-grafana-dashboard-1.png[HEMS system set-up]

Let's walk through the different elements in this diagram one by one.

==== The electricity system

The electricity system in the house is comprised of:

- A connection to the electricity grid and depicted as a lightning bolt.

- The electricity meter, currently an analog one and that will be replaced by a digital version sometime between now and the end of 2024. Note that this meter measures real power and it will turn backwards when the energy production is higher than the consumption.

> Note: in Belgium, all domestic consumers will be required to have a digital electricity meter by January 1st, 2025.

==== The _Smappee_ systems

- A _Smappee Solar_ that measures total energy consumption and total solar energy production.

- A _Smappee Infinity_ that measures energy consumption of different electrical devices or groups thereof. For example, the consumption of the electrical furnace and dish washer are measured individually whereas wall sockets are grouped already when they were installed at the time the house was built.

==== The Home Energy Monitoring System

- A Raspberry Pi 4 Model B/8GB with a 250GB SSD (SATA disk connected to one of the Pi's USB-3 ports via a USB-SATA converter).

- The software components &#8212; EMQX MQTT Broker, Telegraf, InfluxDB server, and Grafana Server with data flowing from left to right.

- An _MQTT_ client &#8212; mainly used during debugging MQTT/Telegraf configuration. The EMQX project has an _MQTT_ client with a Graphical User Interface named https://github.com/emqx/MQTTX[MQTTX], but due to it being pretty slow, I switched to https://github.com/eclipse/mosquitto[Mosquitto] CLI.

- Finally, I added the Raspberry Pi OS Metrics Telegraf configuration and corresponding Grafana Dashboard to collect and display Operating System metrics such as system uptime, CPU, Memory, and disk usage.

With the architecture, the physical implementation, and the software component selection out of the way, we will look in the next chapters at how the latter are configured and how a system is provisioned in a reproducible way.

== System provisioning

It is a well established fact that the Internet provides a wealth of information about setting-up IT infrastructure and software. On the other hand, if you ever tried to implement a system by looking up stuff via StackOverflow, blog articles, etc., you probably went through a frustrating experience of dealing with incomplete information, resolving incompatibilities between different version of software, etc.

For example, take the _absolutely great_ https://grafana.com/grafana/dashboards/10578-raspberry-pi-monitoring[Raspberry Pi Grafana dashboard] developed by Jorge de la Cruz. I installed and configured this component before tackling the energy monitoring part. When I added the latter, the Raspberry Pi monitoring dashboard stopped working. An analysis showed that the Telegraf configuration for the RPi monitoring was too generic.

Another example is linked to determining which versions of the software components are supported on a particular version of an operating system. As I am using Ubuntu Server OS, two versions, 20.04 and 22.04 were good candidates, with a preference for version 22.04. Unfortunately, at the time I installed the system, EMQX was only supported on 20.04, which made 22.04 a no-go (at the time of writing https://www.emqx.io/docs/en/v5.0/deploy/install.html#supported-operating-systems[EMQX on Ubuntu 22.04] _is_ supported ).

A way to avoid having to go through a debugging cycle when provisioning a system is to use tools to automate the process as much as possible. Various alternatives exist, but I decided to go for https://cloudinit.readthedocs.io/en/latest/index.html[_cloud-init_].

As _cloud-init_ is bundled with Ubuntu Server, we can use it to our advantage. In that way, we can not only provision a new system with a configuration that is known to work in a reproducible way, but we can do so in the fastest way possible (at least an order of magnitude faster than a manual installation and configuration): the _HEMS_ system can be provisioned in the time span of about 12 minutes (that includes the time required to flash the SD card which is taking a considerable fraction of the total time). In fact, after having provisioned a system through several iterations, I found out that it can be optimised further after realising that even though Ubuntu 20.04 doesn't support booting of an external SSD, it still does so provided that there's a bootable SD card installed on the Pi. The net effect is that the SD card needs to be flashed just once and only the SSD needs to be re-flashed. Given that it takes about 18 seconds to do this, the provisioning process is shortened by several minutes.

Ubuntu 22.04 supports direct booting of an external SSD obviating the need to have an SD card installed on the Pi.

Upgrading my current production system to the latest and greatest is planned somewhere down the line. See the section on <<future-work>> for a complete list of things I have in mind.

== Configuring _MQTT_ on the _Smappees_

A _Smappee_ can be configured to send its measurements to an _MQTT Broker_. This is done in the advanced configuration menu of the _Smappee_. For this configuration, we need the IP address of the broker and the port number it is listening on (default = 1883).

The following screenshot shows the advanced configuration screen of one of the _Smappees_.

image::smappee_mqtt_config_1.png[Smappee advanced configuration screen]

Note that the configuration set the broker's IP address to 192.168.68.201, the port number to 1883, and the transport layer communication protocol to TCP.

With that configuration change, each _Smappee_ will now start sending _MQTT_ data to the broker. If the configuration is incorrect (e.g. wrong IP address or port number), data will be lost. Also, if for some reason, the broker is down or unreachable, again data will be lost.

_MQTT_ sends data on so-called MQTT topics. Different options for encoding the actual data exist, but in the case of _Smappee_, JSON encoding is used.

The structure of the data is different between the _Smappee Solar_ and the _Smappee Genius_. Let's look at both of them, starting with the Solar:

```bash
$ mosquitto_sub -h 192.168.68.201 -p 1883 -t servicelocation/f960f45d-c43b-4937-a8d0-ce1869206011/realtime| jq
{
  "totalPower": 255,
  "totalReactivePower": 251,
  "totalExportEnergy": 0,
  "totalImportEnergy": 807413694,
  "monitorStatus": 0,
  "utcTimeStamp": 1683799083538,
  "channelPowers": [
    {
      "ctInput": 1,
      "power": 1175,
      "exportEnergy": 6848910,
      "importEnergy": 884498523,
      "phaseId": 1,
      "current": 49
    },
    {
      "ctInput": 2,
      "power": 255,
      "exportEnergy": 0,
      "importEnergy": 807413694,
      "phaseId": 2,
      "current": 15
    }
  ],
  "voltages": [
    {
      "voltage": 241,
      "phaseId": 0
    }
  ]
}
```

On this device, the data we're interested in are the line voltage (`voltages/voltage`), the two power readings (`channelPowers/power` for `channelPowers.ctInput = 1` and `channelPowers.ctInput = 2`) ,and the timestamp of the measurement. We will explain later how this data is extracted and transformed before writing it to InfluxDB.

For the _Smappee Genius_, the (abbreviated) data looks as follows:

```bash
$ mosquitto_sub -h 192.168.68.201 -p 1883 -t servicelocation/5aaf6e89-89cb-4e33-bf34-05abc62f5563/realtime| jq
{
  "totalPower": 0,
  "totalReactivePower": 0,
  "totalExportEnergy": 5900400,
  "totalImportEnergy": 3332883600,
  "monitorStatus": 0,
  "utcTimeStamp": 1683799714000,
  "measuredFrequency": 49983008,
  "channelPowers": [
    {
      "publishIndex": 0,
      "formula": "$5500053415/0$",
      "power": 84,
      "exportEnergy": 2188800,
      "importEnergy": 280227600,
      "phaseId": 0,
      "current": 4,
      "apparentPower": 90,
      "cosPhi": 93
    },
    {
      "publishIndex": 1,
      "formula": "$5500053415/1$",
      "power": 7,
      "exportEnergy": 900000,
      "importEnergy": 277783200,
      "phaseId": 0,
      "current": 1,
      "apparentPower": 21,
      "cosPhi": 33
    },    
    <elided>
  ],
  "voltages": [
    {
      "voltage": 242,
      "phaseId": 0
    },
    <elided>
  ]
}
```

The _Smappee Genius_ collects more information than its smaller sibling. Observe the `measuredFrequency` measurement (expressed in µHz) which allows us track the mains AC frequency, `channelPowers.cosPhi`, measures the so-called <<cos-phi>> on a per channel basis. Interesting to note is the presence of the `channelPowers.formula` value which is the _CT Hub_ Id, a 10-digit number that uniquely identifies each _CT Hub_, followed by a `/` and the channel Id, a number between 0 and 3. All of this encapsulated between two `$` signs.

The following diagram shows the physical configuration & measurement points on the _Smappee Infinity" system.

image::smappee-connection-diagram.png[Smappee connection diagram]

We recognise the four _CT Hubs_ with their respective Id and what is measured on each channel _CT Hub_ channel. The labels _Ground Floor_ and _2nd Floor_ at the top of the diagram refer to the location of the fuse panel in which the _Smappee_ components are located.

Now that we know the message format of the raw data published via MQTT, we will look at how we can get the messages into the Time series database.

=== Configuring Telegraf

Telegraf offers a series of plugins that fall into different categories. For our domain, the plugins that are of interest are:

* Input: https://docs.influxdata.com/telegraf/v1.26/plugins/#input-mqtt_consumer[MQTT Consumer] and the https://docs.influxdata.com/telegraf/v1.21/data_formats/input/json_v2[JSON v2 parser]. Note that the latter is a generic component that can be applied on any input plugin.
* Aggregator: https://docs.influxdata.com/telegraf/v1.26/plugins/#aggregator-basicstats[Basic Stats]
* Processor: https://docs.influxdata.com/telegraf/v1.26/plugins/#processor-regex[Regex] 
* Output: https://docs.influxdata.com/telegraf/v1.23/plugins/#output-influxdb[InfluxDB v1.x]

==== Reading and transforming the MQTT data sources

Our two _Smappees_ send data to the _EMQX_ _MQTT_ broker located at `tcp://192.168.68.201:1883`.

Let's look at the relevant part of the (partial) Telegraf configuration for the _Smappee Genius_.

```toml
[[inputs.mqtt_consumer]]
  alias = "smappee-2"
  name_override = "smappee-data-2"
  servers = ["tcp://192.168.68.201:1883"]
  topics = [
    "servicelocation/5aaf6e89-89cb-4e33-bf34-05abc62f5563/realtime"
  ]
  # The "host" tag is irrelevant in this use case. Drop it
  tagexclude = ["host"]
  data_format = "json_v2"
  [[inputs.mqtt_consumer.json_v2]]
    [[inputs.mqtt_consumer.json_v2.field]]
      path = "channelPowers.#(formula==$5500048161/0$).power"
      rename = "zolder-verlichting"
      type = "float"
    [[inputs.mqtt_consumer.json_v2.field]]
      path = "channelPowers.#(formula==$5500048161/1$).power"
      rename = "zolder-stopkontakten-network-switch"
      type = "float"
    [[inputs.mqtt_consumer.json_v2.field]]
      path = "channelPowers.#(formula==$5500048161/2$).power"
      rename = "tuinhuis-fietsgarage"
      type = "float"
      
  <elided>
```

We are configuring the `mqtt_consumer` input plugin and point it to connect to the EMQX broker. The `topics` settings is used to instruct the plugin to subscribe to the _MQTT_ topic of interest. Next, the `name_override` setting is used to name the stream of data elements produced by the input plugin. This name is used to select the desired route that the data will follow as it is processed by other plugins. Finally, we tell the plugin that the data is in JSON format (`json_v2`) and we drop the `host` field from the data as otherwise, the data will be tagged with the hostname of the host on which Telegraf is running.

We're ready to configure the JSON parser, which is done in the `inputs.mqtt_consumer.json_v2` configuration section. For each field in the data that we want to retain for further processing, there's a section that selects the field, renames it, and specifies its format.

You may wonder how one knows the syntax of the `path` selector. A very handy tool for this is the https://gjson.dev[GJSON playground] which allows one to try out queries on JSON data. It comes with examples for the most important use cases.

Here are two examples of queries on the _Smappee Genius_ data. These respectively extract the `measuredFrequency` value and the `power` value for channel 0 on the _CT Hub_ with Id `5500053415`.

image::GJSON-measured-frequency.png[GJSON - Extracting power for channel 0 on CT Hub 5500053415]  

image::GJSON-measured-power.png[GJSON - Extracting measuredFrequency]

==== Transforming the _MQTT_ topic

If we would limit the Telegraf configuration to what we have up to now, the data would be tagged with the rather lengthy topic (`servicelocation/5aaf6e89-89cb-4e33-bf34-05abc62f5563/realtime`). It makes sense to drop the `servicelocation` and the `realtime` parts. This can be done using the _regex_ processor by adding the following configuration:

```toml
[[processors.regex]]
  namepass = ["smappee-data-2"]
  [[processors.regex.tags]]
    key = "topic"
    pattern = ".*/(.*)/.*"
    replacement = "smappee/${1}"
```

The following can be observed:

* By specifying the `namepass` setting, the processor will only apply to the data we want to transform. If we would leave it out, the transformation would be applied on _all_ data.
* We select the `topic` key and apply a pattern match on its value via a regular expression which captures the value of the second field.
* The original topic value, `servicelocation/5aaf6e89-89cb-4e33-bf34-05abc62f5563/realtime`, is replaced by the new value `smappee/5aaf6e89-89cb-4e33-bf34-05abc62f5563`.

==== Aggregating the data

Writing the measurements at the _Smappee_ 1Hz sample rate is overkill, so we want to aggregate measurements on a longer interval. I went for retaining average values over 1 minute intervals. This can be implemented using the _basicstats_ Telegraf aggregator plugin.

Here's the configuration for this:

```toml
[[aggregators.basicstats]]
  namepass = ["smappee-data-2"]
  ## The period on which to flush & clear the aggregator.
  period = "60s"

  ## If true, the original metric will be dropped by the
  ## aggregator and will not get sent to the output plugins.
  drop_original = true

  ## Configures which basic stats to push as fields
  stats = ["mean"]
```

The usage of the `namepass` setting should be familiar by now. Other than that, we set the aggregation interval to 60 seconds (setting `period`), we drop the original (1 second) measurements, as we only want the plugin to calculate the average value via the `stats` setting.

We could also choose to let Telegraf handle further aggregation to longer intervals, but that's a task that is better left to InfluxDB as the latter will also help use to specify data retention times.

All that's left to do is to write the data to the Time series database.

==== Writing the processed data to InfluxDB

An InfluxDB server is running on the same host (`http://192.168.68.201:8086`).

All that is needed now is adding this configuration for the Telegraf InfluxDB output plugin:

```toml
[[outputs.influxdb]]
  namepass = ["smappee-data-2"]
  alias = "smappee-out-2"
  urls = ["http://192.168.68.201:8086"]
  database = "smappee_monitoring_2"
  username = "this is not my username"
  password = "this is not my password"
```

This configuration is for InfluxDB version 1. As can be seen, the database username & password are passed in the config which is not best practice... This will be revisited as part of a future migration to InfluxDB version 2 which has a completely revised security implementation.

==== Lessons learned from setting up Telegraf and InfluxDB

===== Telegraf - message routing through plugins

The Telegraf plugin system is very powerful, but it took me quite some time to wrap my head around its configuration. Even though there are video tutorials and online courses on various Telegraf related topics, it took me a lot of time to grasp how data is routed through the system by applying the `name_override`, `namepass`, and `namedrop` parameters. When it finally dawned on me how it works, it looked trivial (and it actually _is_).

===== Telegraf - plugin application order

The order of application of Telegraf plugins is determined by the plugin type. Obviously, _Input_ plugins are applied first, _Output_ plugins are applied last.

_Processor_ plugins are applied after the _Input_ plugins have been applied and _before_ any _Aggregator_ plugin is applied. For _Processor_ plugins, the order of execution can be tweaked by setting the `order` parameter on all processors involved.

The https://github.com/influxdata/telegraf/blob/master/docs/CONFIGURATION.md[Telegraf configuration document] is worth reading and provides a lot of very useful information that you may want to read before embarking for the first time on a Telegraf project.

==== Telegraf - conclusion

The HEMS has a relatively simple Telegraf configuration. The configuration can be put in a single file (`/etc/telegraf/telegraf.conf`), or across multiple files located in the `/etc/telegraf/telegraf.d` folder. An advantage of using multiple configuration files is that the configs for the two `Smappee` systems can be separated. In fact, when I recently added some Zigbee 3.0 devices, that connect to a _Zigbee2MQTT_ bridge configured in a Home Automation system, its Telegraf configuration was stored in a dedicated file.

One thing to be aware of is that using multiple configuration files doesn't introduce any separation between the individual configs, so treat it as if everything was stored in a  single file.

A cool feature of Telegraf is that the `telegraf` CLI can generate a configuration file for a list of Telegraf plugins with all possible setting applicable to the chosen plugins.

I think that in a complex system, it can be challenging to maintain the Telegraf configuration(s). InfluxDB version 2 probably has features that simplify managing this, but that's something I haven't explored in detail yet.

==== InfluxDB

Installation and configuration of InfluxDB version 1 is simple. The installation was automated using _cloud-init_, including the creation of the user databases & user account.

I didn't spend a lot of time trying to make the set-up secure as I think that InfluxDB version 2 has a lot more to offer in this domain.

Actually, when I started this project, I wasn't aware of the fact that there is a version 2 of InfluxDB. I found out by the time the project was already long underway. I did a small trial by uninstalling version 1 followed by an installation of version 2. What I found impressive is that when I first started the new version 2 instance, it told me that it had detected version 1 databases and if I wanted them converted to version 2. I accepted the offer and it worked flawlessly. What I liked even more is that when I reverted the installation to version 1, my original data was still there and ready to continue where I left off. Pretty impressive if you ask me.

=== Provisioning the _HEMS_ system with _cloud-init_

_cloud-init_ is a method for cross-platform instance initialisation. Even though it is mostly used on Cloud based deployments, it can be used for bare-metal installations like on a Raspberry Pi. It performs user creation, execute custom scripts, install packages, create files, partition disk, create file systems, etc.

It used to have rather poor documentation, but this is a thing of the past. When you want to start with _cloud-init_ have at the https://cloudinit.readthedocs.io/en/latest/reference/examples.html[Cloud Config Examples] which should get you started quickly. These examples are part of the https://cloudinit.readthedocs.io/en/latest/reference/index.html[Reference Documentation] on the https://cloudinit.readthedocs.io/en/latest/[_cloud-init_ website].

==== Using _cloud-init_

I started using _cloud-init_ many years ago on another Raspberry Pi project. Back then, I used the _Hypriot_ operating system (a derivative of https://www.raspberrypi.com/software[Raspbian]) which has integrated support for _cloud-init_ (and _Docker_). The Hypriot OSS project has gone dormant since a fewyears, but one of the contributors pointed out that Ubuntu Server has the same goodies incorporated. So, I switched to Ubuntu and never looked back.

A _cloud-init_ deployment is driven by a cloud-config file in YAML format. The configuration for this project can be found https://github.com/eloots/home-energy-monitoring-system-setup/blob/main/cloud-init/smappee-2.yml[here]. It's part of the _HEMS_ GitHub repository that also contains the Telegraf configuration https://github.com/eloots/home-energy-monitoring-system-setup/blob/main/telegrafConfiguration/etc/telegraf/telegraf.d/smappee-2.conf[smappee-2.conf]  for the _Smappee Genius_ and https://github.com/eloots/home-energy-monitoring-system-setup/blob/main/telegrafConfiguration/etc/telegraf/telegraf.d/smappee.conf[smappee.conf] for the _Smappee Solar_.

Noteworthy mentioning is that _cloud-init_ has support for instance data with [jinja] template rendering. Instead of customising instance specific configuration setting in the cloud-config file, metadata consisting of key/value pairs can be passed to _cloud-init_ in a file and these can be dereferenced in the cloud-config file.

In the case of the Ubuntu _cloud-init_ installation, an adapted version of the _Hypriot_ `flash` command supports passing in the metadata file during flashing. This version can be found https://github.com/eloots/flash/releases[here].

Here's an example invocation of the command to flash an SSD (or SD) with Ubuntu 22.04

```bash
$ flash -n home-iot -j -m cloud-init/meta-data -u cloud-init/smappee-2.yml \
  https://cdimage.ubuntu.com/releases/22.04/release/ubuntu-22.04.2-preinstalled-server-arm64+raspi.img.xz
```

=== Conclusion

The <<project-objectives>> set at the start of the project were all achieved.

No programming was required to build the _HEMS_, all software components are OSS, and a new _HEMS_ system can be provisioned in a matter of minutes.

The only thing that needs to be done after a new _HEMS_ is provisioned is to point the _MQTT_ data providers to it. Finally, and that's about the only manual step, Grafana needs to be configured and dashboards must be imported. If you decide to replicate this set-up in your house, this will require you to adapt the Telegraf config & Grafana dashboards to your specific configuration.

<<<
[id=cos-phi]
=== Electrical Power Factor (also known as cos(&#966;))

The Power Factor is a measure of an electrical system's efficiency. In that respect, the power consumed can be decomposed in three parts:

* P~a~: Apparent power, which is the product of the measured voltage and current.

* P~real~: Productive or Real Power, is the part that actually produces work (in the broad sense of the word, it's not the internal efficiency of the device itself. For example, an incandescent lightbulb consuming 100W, only converts about 2W into visible light whereas  98W is converted to heat. So, in this case, the Productive Power is 100W)

* P~reactive~: Reactive power (VAR - Volt-Ampère-Reactive) which is the part that doesn't perform any work, but that still flows between the electricity producer and consumer.

The relation between these components is the following:

* P^2^~a~ = P^2^~real~ + P^2^~reactive~

* P~real~ = P~a~ . cos(&#966;)

* P~reactive~ = P~a~ . sin(&#966;)

In electrical systems, cos(&#966;) is a value between 0 and 1. When the reactive power is 0, cos(&#966;) is equal to 1 and P~a~ and P~real~ have equal values. On the other extreme, P~reactive~ is equal to P~a~, and P~real~ and cos(&#966;) are both 0.

Both the real- and the reactive power components transfer energy between energy producer and consumer. The core distinction between the two is that the real power component unidirectionally transfers energy from producer to consumer whereas the reactive power component corresponds to energy being bounced back and forth between producer and consumer resulting in no effective work being done on the consumer side.

If we look at the larger picture though, both are transferred through the grid, via high voltage transmission lines, transformers, and local power distribution systems. During this transfer, losses occur amounting to 6% to 8% of the total energy produced. Because the reactive power flows through the grid, it contributes to losses for no useful purpose, and it increases the total current flowing through wires which leads to the need to use thicker wires which in turn increases cost to build the grid. Companies that produce and sell electricity want the reactive power to be 0 or very small compared to the apparent power. This because the reactive power can be seen as power that needs to be generated and transported and that doesn't do any useful work, and in principle isn't billable to the consumer. With the explosion of devices with internal switching power supplies such as mobile phones, laptop computers, etc., this poses a real challenge as these supplies exhibit a poor cos(&#966;). Regulations are being put in place  to force manufacturers to address this issue.

When measuring the cos(&#966;) of various equipment in the house, I noticed that the charger in the electrical car has a cos(&#966;) that is pretty much equal to 1, which means that it exhibits close to perfect behaviour. This is especially important as the power consumed during the charging is in the order of multiple kilo Watts. On the other hand, the charger of our electrical bicycle charger's cos(&#966;) is only 0.69. So, when charging, the different powers are P~real~ = 180W, P~a~ = 260VA, and P~reactive~ = 187W. Even though it's "only 187W", when thousand or millions of such devices are online, the impact _is_ significant.  

<<<
[id=future-work]
=== Future work

* Upgrade of the production system without any loss of data
** Upgrade Ubuntu 20.04 to Ubuntu 22.04
** Upgrade InfluxDB v1 to InfluxDB v2
* Add pricing data to the system
** Add dashboards that display the price of electricity imported from (and exported to) the grid over a specified period and for specific consumers
* Actively control electricity consumption to:
** Reduce peak consumption
** Drive down the electricity bill by shifting consumption to moments where the prices are lower. Candidate consumers are electrical water boilers and electric cars



